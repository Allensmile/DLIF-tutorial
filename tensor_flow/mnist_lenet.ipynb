{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN example with TensorFlow\n",
    "\n",
    "In this example, we classify hand-written digits of MNIST dataset with LeNet5. This example is based on the official example in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "This example takes the following steps:\n",
    "\n",
    "1. Import packages\n",
    "2. Prepare dataset\n",
    "3. Prepare model and optimizer\n",
    "4. Initialize parameters\n",
    "5. Run training loop\n",
    "6. Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_Data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_Data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_Data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_Data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_Data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Allocate the tensors on GPU 0\n",
    "with tf.device('/gpu:0'):\n",
    "    # MNIST consists of 28x28 gray scale images,\n",
    "    # each represented as a 784 dimensional vector.\n",
    "    # None indicates the size of minibatches, determined on exeution\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    t = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # -1 indicates the size of minibatches, determined on execution\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # layer 1: 5x5 convolution, tanh, 2x2 max pooling\n",
    "    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 6], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[6]))\n",
    "    a1 = tf.nn.tanh(tf.nn.conv2d(x_image, W1, strides=[1, 1, 1, 1], padding='SAME') + b1)\n",
    "    h1 = tf.nn.max_pool(a1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # layer 2: 5x5 convolution, tanh, 2x2 max pooling\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, 6, 16], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[16]))\n",
    "    a2 = tf.nn.tanh(tf.nn.conv2d(h1, W2, strides=[1, 1, 1, 1], padding='VALID') + b2)\n",
    "    h2 = tf.nn.max_pool(a2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # layer 3: fully connected\n",
    "    h2_flat = tf.reshape(h2, [-1, 5 * 5 * 16])\n",
    "    W3 = tf.Variable(tf.truncated_normal([5 * 5 * 16, 120], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.constant(0.1, shape=[120]))\n",
    "    h3 = tf.nn.tanh(tf.matmul(h2_flat, W3) + b3)\n",
    "    \n",
    "    # layer 4: fully connected\n",
    "    W4 = tf.Variable(tf.truncated_normal([120, 84], stddev=0.1))\n",
    "    b4 = tf.Variable(tf.constant(0.1, shape=[84]))\n",
    "    h4 = tf.nn.tanh(tf.matmul(h3, W4) + b4)\n",
    "    \n",
    "    # layer 5: fully connected\n",
    "    W5 = tf.Variable(tf.truncated_normal([84, 10], stddev=0.1))\n",
    "    b5 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    h5 = tf.matmul(h4, W5) + b5\n",
    "    \n",
    "    # Compute the loss value\n",
    "    y = tf.nn.softmax(h5)\n",
    "    cross_entropy = -tf.reduce_sum(t * tf.log(y))\n",
    "    \n",
    "    # Update formula\n",
    "    train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# We can put operators on different devices just by\n",
    "# using the 'with' statement.\n",
    "with tf.device('/cpu:0'):\n",
    "    # Compute accuracy for evaluation\n",
    "    correct_or_not = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_or_not, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How convolution and pooling layers handle merginals in TensorFlow.\n",
    "\n",
    "Convolution and padding layers sweep over the input tensors with kernels in forward propagation. The output of layers are determined by input size, kernel size, stride, which is the distance kernels move at one step, and finally, how layers handle merginals of input data. \n",
    "\n",
    "There two ways to handle the merginals, which are specified by ``padding`` argument. One is ``VALID``, in which residual locations are discarded. The other is ``SAME``, in which the both side of the input is equally padded with 0 so that all locations are convolved.\n",
    "\n",
    "```\n",
    "e.g. width = 9, kernel size = 4, stride = 2\n",
    "\n",
    "* padding = 'VALID'\n",
    "  1 2 3 4 5 6 7 8 9\n",
    "  |-----|\n",
    "      |-----|\n",
    "          |-----|    \n",
    "\n",
    "* padding = 'SAME'\n",
    "0 1 2 3 4 5 6 7 8 9 0 0\n",
    "|-----|\n",
    "    |-----|\n",
    "        |-----|\n",
    "            |-----|\n",
    "                |-----|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize parameters\n",
    "\n",
    "Once the computational graph is built, we can execute the graph using ``Session``. ``Session.run`` takes a list of _fetches_ as the first argument, which indicates the objective node to compute.\n",
    "\n",
    "Before the training starts, we have to initialize parameters. This can be done by the special operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run training loop\n",
    "\n",
    "We can now enter the learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 train accuracy: 0.00021999999880790712\n",
      "step 0 test accuracy: 0.09260000005364417\n",
      "step 500 train accuracy: 0.9054000027477741\n",
      "step 500 test accuracy: 0.9637000048160553\n",
      "step 1000 train accuracy: 0.9698000077009201\n",
      "step 1000 test accuracy: 0.9772000122070312\n",
      "step 1500 train accuracy: 0.9779400104284286\n",
      "step 1500 test accuracy: 0.9819000118970871\n",
      "step 2000 train accuracy: 0.9806600105762482\n",
      "step 2000 test accuracy: 0.9829000109434127\n",
      "step 2500 train accuracy: 0.98526001060009\n",
      "step 2500 test accuracy: 0.9848000115156174\n",
      "step 3000 train accuracy: 0.9872400100231171\n",
      "step 3000 test accuracy: 0.9856000089645386\n",
      "step 3500 train accuracy: 0.9889800088405609\n",
      "step 3500 test accuracy: 0.9879000079631806\n",
      "step 4000 train accuracy: 0.9905800082683563\n",
      "step 4000 test accuracy: 0.9881000101566315\n",
      "step 4500 train accuracy: 0.991140007853508\n",
      "step 4500 test accuracy: 0.9873000109195709\n",
      "step 5000 train accuracy: 0.9924200071096421\n",
      "step 5000 test accuracy: 0.9883000087738038\n",
      "step 5500 train accuracy: 0.9932400060892105\n",
      "step 5500 test accuracy: 0.9880000108480453\n",
      "step 6000 train accuracy: 0.9942200055122375\n",
      "step 6000 test accuracy: 0.9881000083684921\n",
      "step 6500 train accuracy: 0.9954800043106079\n",
      "step 6500 test accuracy: 0.9883000099658966\n",
      "step 7000 train accuracy: 0.9957400040626526\n",
      "step 7000 test accuracy: 0.9887000095844268\n",
      "step 7500 train accuracy: 0.996420003414154\n",
      "step 7500 test accuracy: 0.9873000103235244\n",
      "step 8000 train accuracy: 0.9967800030708313\n",
      "step 8000 test accuracy: 0.9882000082731247\n",
      "step 8500 train accuracy: 0.9974200024604797\n",
      "step 8500 test accuracy: 0.9888000071048737\n",
      "step 9000 train accuracy: 0.9980600018501282\n",
      "step 9000 test accuracy: 0.9880000108480453\n",
      "step 9500 train accuracy: 0.9980200018882751\n",
      "step 9500 test accuracy: 0.9884000104665757\n"
     ]
    }
   ],
   "source": [
    "accum_acc = 0\n",
    "for i in range(10000):\n",
    "    x_batch, t_batch = mnist.train.next_batch(100)\n",
    "    _, acc = sess.run([train_step, accuracy], feed_dict={x: x_batch, t: t_batch})\n",
    "    accum_acc += acc\n",
    "    if i % 500 == 0:\n",
    "        print('step {} train accuracy: {}'.format(i, accum_acc / 500))\n",
    "        accum_acc = 0\n",
    "        \n",
    "        # Evaluate the model on the test dataset\n",
    "        for _ in range(100):\n",
    "            x_batch, t_batch = mnist.test.next_batch(100)\n",
    "            accum_acc += sess.run(accuracy, feed_dict={x: x_batch, t: t_batch})\n",
    "        print('step {} test accuracy: {}'.format(i, accum_acc / 100))\n",
    "        accum_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, we can save the model with ``Saver``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnist_lenet5-10000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'mnist_lenet5', global_step=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
