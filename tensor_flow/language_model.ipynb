{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN example with TensorFlow\n",
    "\n",
    "In this example, we build the language model made of stacked LSTMs with Penn Treebank dataset. This notebook is based on the official tutorial of TensorFlow on language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "This example takes the following steps:\n",
    "\n",
    "1. Import packages\n",
    "2. Prepare dataset\n",
    "3. Prepare model, optimizer, and parameter initializer\n",
    "4. Run training loop\n",
    "5. Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before executing the code, please download dataset from [Mikolov's website](http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz) and decompress it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below show how to create a minibatch in this example. The raw input is a sequence of integers, which represents word IDs. We first reshape it to 2-dimensional matrix with as many rows as the minibatch size. Then, extract the minibatch of feature vector from columns ``[i : i+L]``, and of target vectors from ``[i+1 : i+L+1]`` where ``i`` is the iteration count and ``L`` is the BPTT length.\n",
    "\n",
    "![How to create minibatch](../image/tf_rnn_minibatch.png)\n",
    "Fig. How to create a minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    \"\"\"The input data.\"\"\"\n",
    "\n",
    "    def __init__(self, config, data, name):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(\n",
    "            data, batch_size, num_steps, name=name)\n",
    "\n",
    "raw_data = reader.ptb_raw_data('simple-examples/data/')\n",
    "train_data, valid_data, test_data, _ = raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare model, optimizer, and parameter initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``PTBModel`` defines the RNN architecture and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, is_training, config, input_):\n",
    "        self._input = input_\n",
    "\n",
    "        batch_size = input_.batch_size\n",
    "        num_steps = input_.num_steps\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "\n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(\n",
    "                size, forget_bias=0.0, state_is_tuple=True)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [lstm_cell() for _ in range(config.num_layers)],\n",
    "            state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # Forward propagation\n",
    "        # Word embedding\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, size], dtype=tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "\n",
    "        # RNN\n",
    "        inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, inputs,\n",
    "                                                  initial_state=self._initial_state)\n",
    "\n",
    "        # Linear\n",
    "        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, size])\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(input_.targets, [-1])],\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # Backward propagation\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "\n",
    "        # Parameter update\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        # Adjustment of learning rate\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training configurations\n",
    "class SmallConfig(object):\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "\n",
    "config = SmallConfig()\n",
    "eval_config = SmallConfig()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup models for training/validation/testing with ``PTBInput`` and ``PTBModel``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                            config.init_scale)\n",
    "\n",
    "# Setup the model for training\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "    # Prepare initializer\n",
    "    with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "    tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "    tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "# Setup the model for validation\n",
    "with tf.name_scope(\"Valid\"):\n",
    "    valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "    # Parameters in the model is shared with the training model.\n",
    "    with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "    tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "\n",
    "# Setup the model for testing\n",
    "with tf.name_scope(\"Test\"):\n",
    "    test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "    # Parameters in the model is shared with the training model.\n",
    "    with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mtest = PTBModel(is_training=False, config=eval_config,\n",
    "                         input_=test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training loop and 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None):\n",
    "    \"\"\"Execute single epoch.\"\"\"\n",
    "    \n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    # initialize RNN states\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        # Process one minibatch\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "Epoch: 1 Train Perplexity: 270.203\n",
      "Epoch: 1 Valid Perplexity: 184.724\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "Epoch: 2 Train Perplexity: 133.531\n",
      "Epoch: 2 Valid Perplexity: 147.012\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "INFO:tensorflow:Model/global_step/sec: 55.8605\n",
      "Epoch: 3 Train Perplexity: 102.464\n",
      "Epoch: 3 Valid Perplexity: 131.377\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "Epoch: 4 Train Perplexity: 86.059\n",
      "Epoch: 4 Valid Perplexity: 126.897\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "Epoch: 5 Train Perplexity: 65.631\n",
      "Epoch: 5 Valid Perplexity: 118.964\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "INFO:tensorflow:Model/global_step/sec: 55.0667\n",
      "Epoch: 6 Train Perplexity: 53.493\n",
      "Epoch: 6 Valid Perplexity: 118.570\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "Epoch: 7 Train Perplexity: 47.197\n",
      "Epoch: 7 Valid Perplexity: 119.952\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "INFO:tensorflow:Model/global_step/sec: 42.8499\n",
      "Epoch: 8 Train Perplexity: 44.064\n",
      "Epoch: 8 Valid Perplexity: 120.727\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "Epoch: 9 Train Perplexity: 42.388\n",
      "Epoch: 9 Valid Perplexity: 121.090\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "Epoch: 10 Train Perplexity: 41.488\n",
      "Epoch: 10 Valid Perplexity: 120.973\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "INFO:tensorflow:Model/global_step/sec: 53.4251\n",
      "Epoch: 11 Train Perplexity: 40.997\n",
      "Epoch: 11 Valid Perplexity: 120.685\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "Epoch: 12 Train Perplexity: 40.712\n",
      "Epoch: 12 Valid Perplexity: 120.459\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "Epoch: 13 Train Perplexity: 40.565\n",
      "Epoch: 13 Valid Perplexity: 120.308\n",
      "INFO:tensorflow:Model/global_step/sec: 44.4749\n",
      "Test Perplexity: 114.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Model/global_step/sec: 55.8605\n",
      "INFO:tensorflow:Model/global_step/sec: 55.0667\n",
      "INFO:tensorflow:Model/global_step/sec: 42.8499\n",
      "INFO:tensorflow:Model/global_step/sec: 53.4251\n",
      "INFO:tensorflow:Model/global_step/sec: 44.4749\n"
     ]
    }
   ],
   "source": [
    "sv = tf.train.Supervisor(logdir='result')\n",
    "with sv.managed_session() as session:\n",
    "    for i in range(config.max_max_epoch):\n",
    "        lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
    "\n",
    "        # Change learning rate\n",
    "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "\n",
    "        # Execute one sweep of training dataset\n",
    "        train_perplexity = run_epoch(session, m, eval_op=m.train_op)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "\n",
    "        # Evaluation with validation dataset\n",
    "        valid_perplexity = run_epoch(session, mvalid)\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "    # Evaluation with test dataset\n",
    "    test_perplexity = run_epoch(session, mtest)\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "    sv.saver.save(session, 'result', global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
