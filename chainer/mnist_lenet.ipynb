{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN example with Chainer\n",
    "\n",
    "In this example, we classify hand-written digits of MNIST dataset with LeNet5. This example is based on the official example in Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "The MNIST dataset is a dataset of images of hand written digits. It consists of 60000 samples for training and 10000 for testing. Each sample is a gray scale image whose size is normalized to 28x28. One of single digit out of ten is written in each image. The task is to classify the number written from the image.\n",
    "\n",
    "![Example of MNIST](../image/mnist.png)\n",
    "Fig. Example of the image in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet5\n",
    "\n",
    "LeNet5 is one of the most famous CNN architecture proposed by LeCun et al. in [LeCun+98].\n",
    "\n",
    "![The architecture of LeNet5](../image/lenet5.png)\n",
    "Fig. The architecture of LeNet5 (cited from [LeCun+98]).\n",
    "\n",
    "\n",
    "[LeCun+98]: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*, 86(11), 2278-2324."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Chainer\n",
    "\n",
    "**chainer.Variable**\n",
    "\n",
    "* Data nodes of computational graphs.\n",
    "* Wrapper for ``numpy.ndarray`` and ``cupy.ndarray``\n",
    "\n",
    "**chainer.Function**\n",
    "\n",
    "* Non-parameterized operator nodes of computational graphs.\n",
    "* e.g. ``F.exp``, ``F.sigmoid``, ``F.LinearFunction``\n",
    "* Fully-connected layers are realized as \\\\(f(x, W, b) = Wx + b\\\\)\n",
    "\n",
    "**chainer.Link**\n",
    "\n",
    "* Parmeterized operator nodes of computational graphs.\n",
    "* e.g. ``L.Linear``, ``L.Convolution2D``\n",
    "* Fully-connected layers are realized as \\\\(f_{W, b}(x) = Wx + b\\\\)\n",
    "* In most case, it internally uses corresponding Function.\n",
    "\n",
    "**chainer.Chain**\n",
    "\n",
    "* An object that bundles ``Link`` instances.\n",
    "* ``Chain`` itself is a derived class of ``Link``.\n",
    "\n",
    "![linear function and linear link](../image/linear.png)\n",
    "Fig. Linear function and linear link\n",
    "\n",
    "\n",
    "**chainer.training.Trainer and related modules**\n",
    "\n",
    "* An object that is responsible for training loops.\n",
    "* Iterates the procedures of forward/backward propagations and parameter update.\n",
    "* Training procedures are customized by trainer-related modules.\n",
    "    * ``Dataset`` and its ``Iterator`` extract mini batches from the dataset.\n",
    "    * ``Updater`` determines how parameters are updated.\n",
    "* Extra procedures (e.g. take snapshot of models etc.) are appended by ``Extension``'s.\n",
    "\n",
    "\n",
    "![The relationship of trainer related modules](../image/trainer_related_modules.png)\n",
    "Fig. Trainer-related modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "This example takes the following steps:\n",
    "\n",
    "1. Import packages\n",
    "2. Prepare dataset\n",
    "3. Prepare model\n",
    "4. Setup optimizer\n",
    "5. Training\n",
    "6. Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes\n",
    "\n",
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "import chainer.training.extensions as E\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer.optimizers as O\n",
    "import chainer.datasets as D\n",
    "from chainer import training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to handle image data as a tensor\n",
    "\n",
    "A mini batch of images are represented as a 4-dimensional tensor in usual frameworks.\n",
    "Each axis represents sample size, channels, height, and width. For example, RGB images have 3 channels, each of which represents, Red, Green, and Blue, respectively. As the MNIST dataset is gray scale, its chennel size is 1. Similarly, the input and output of 2D Convolution layer are also 4-dimensional, whose chennals are not necesarily 1 even if we handle gray scale images.\n",
    "\n",
    "There are two ordering methods of how images data are represented.\n",
    "\n",
    "* ``shape = (samples, channels, height, width) e.g. (B, 1, 28, 28)``\n",
    "* ``shape = (samples, height, width, channels) e.g. (B, 28, 28, 1)``\n",
    "\n",
    "Chainer and Theano use the former format, while TensorFlow the latter one. Keras can switch the ordering depending on the backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = D.get_mnist(ndim=3)  # each image has shape (1, 28, 28)\n",
    "\n",
    "# Get iterators of datasets\n",
    "batchsize = 128\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discrimininator\n",
    "class LeNet5(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__(\n",
    "            # The size of each image of MNIST is 28 x 28,\n",
    "            # but LeNet5 is designed so that its input is 32 x 32.\n",
    "            # So we add the padding of size (32 - 28) / 2 = 2.\n",
    "            conv1=L.Convolution2D(1, 6, 5, pad=2),\n",
    "            conv2=L.Convolution2D(6, 16, 5),\n",
    "            fc1=L.Linear(400, 120),\n",
    "            fc2=L.Linear(120, 84),\n",
    "            fc3=L.Linear(84, 10)\n",
    "        )\n",
    "        self.train = True\n",
    "\n",
    "    # Implementation of forward propagation.\n",
    "    def __call__(self, x):\n",
    "        h = F.tanh(self.conv1(x))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.tanh(self.conv2(h))\n",
    "        h = F.max_pooling_2d(h, 2)\n",
    "        h = F.tanh(self.fc1(h))\n",
    "        h = F.tanh(self.fc2(h))\n",
    "        return self.fc3(h)\n",
    "\n",
    "# Classifier forwards the data to the discriminator to get the prediction\n",
    "# and calculates the loss with the specified loss function.\n",
    "# By default, softmax_cross_entropy loss (softmax followed by cross entropy loss) is used.\n",
    "# Classifier itself is a derived class of Chain.\n",
    "model = L.Classifier(LeNet5())\n",
    "\n",
    "gpu = 0  # GPU ID to use. Negative value to use CPU.\n",
    "if gpu >= 0:\n",
    "    # Transfer the model to GPU\n",
    "    cuda.get_device(gpu).use()\n",
    "    model.to_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training & 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "\n",
    "# Setup trainer\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'))\n",
    "\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer.extend(E.Evaluator(test_iter, model, device=gpu))\n",
    "\n",
    "# Dump a computational graph from 'loss' variable at the first iteration\n",
    "trainer.extend(E.dump_graph('main/loss'))\n",
    "\n",
    "# Take a snapshot at each epoch\n",
    "trainer.extend(E.snapshot(), trigger=(epoch, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer.extend(E.LogReport())\n",
    "\n",
    "# Save two plot images to the result dir\n",
    "trainer.extend(\n",
    "    E.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                 'epoch', file_name='loss.png'))\n",
    "trainer.extend(\n",
    "    E.PlotReport(['main/accuracy', 'validation/main/accuracy'],\n",
    "                 'epoch', file_name='accuracy.png'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "trainer.extend(E.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "# Print a progress bar to stdout\n",
    "# trainer.extend(E.ProgressBar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.267172    0.0726448             0.925423       0.977848                  6.78626       \n",
      "\u001b[J2           0.0686068   0.0469076             0.979777       0.985858                  13.2431       \n",
      "\u001b[J3           0.0467516   0.047265              0.986074       0.984474                  19.6243       \n",
      "\u001b[J4           0.0357922   0.0421441             0.989166       0.986155                  26.0474       \n",
      "\u001b[J5           0.0277272   0.0371696             0.991455       0.988528                  32.4075       \n",
      "\u001b[J6           0.0216469   0.0338254             0.993487       0.988924                  38.8193       \n",
      "\u001b[J7           0.0170145   0.0361768             0.994903       0.988924                  45.2158       \n",
      "\u001b[J8           0.0132753   0.0389417             0.996127       0.987935                  51.6177       \n",
      "\u001b[J9           0.01141     0.0330201             0.996718       0.990012                  58.063        \n",
      "\u001b[J10          0.00995988  0.0354507             0.997035       0.989913                  64.1824       \n"
     ]
    }
   ],
   "source": [
    "# Execute trainer\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
