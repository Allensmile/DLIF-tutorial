{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN example with Chainer\n",
    "\n",
    "In this example, we build the language model made of stacked LSTMs with [Penn Treebank](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model is a probabilistic model over words. It assigns a probability \\\\(p(w)\\\\) to a sequence of words \\\\(w = (w\\_1, \\ldots, w\\_n)\\\\). We model the probability with a Recurrent Neural Network (RNN). Specifically, we decompose the probability as\n",
    "\\\\[p(w) = \\prod\\_{t=1}^{n} p(w\\_t|w\\_1, \\ldots w\\_{t-1})\\\\] and models the conditional probability on the right hand side with the RNN. At time \\\\(t\\\\), the RNN should outputs the probability distribution over words given the previous words \\\\(w\\_1, \\ldots w\\_{t-1}\\\\). The RNN holds the information of previous words as a state, written as \\\\(h\\_t\\\\) at time \\\\(t\\\\). RNN simultaneously outputs the probability distribution of next word and updates the internal state. Schematically,\n",
    "\\\\[(p(w\\_t), h\\_t) = \\mathrm{RNN}(w\\_t, h\\_{t-1}). \\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penn Treebank (PTB)\n",
    "\n",
    "Treebank is a text corpas that annotates syntactic and semantic structure. The Penn Treebank(PTB) is one of the most famous dataset of treebank consists of approximately 4.5 million words. The sentences in the datasets are annotated with POS (part of speech)  taggingIn this tutorial, we do not use the grammatical structure. the just treat the dataset as a bundle of sentences and \n",
    "\n",
    "https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "This example takes the following steps:\n",
    "\n",
    "1. Import packages\n",
    "2. Prepare dataset\n",
    "3. Prepare model\n",
    "4. Setup optimizer\n",
    "5. Training\n",
    "6. Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes\n",
    "\n",
    "### 1. Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.datasets as D\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer.optimizers as O\n",
    "from chainer import training\n",
    "from chainer.training import extensions as E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture show how to create a mini batch from the raw dataset.\n",
    "\n",
    "![How to create minibatch](../image/chainer_rnn_minibatch.png)\n",
    "Fig. How to create a minibatch\n",
    "\n",
    "The raw dataset is a long sequence of integers, each of which corresponds to an ID of single word. We will make a training data that is a list of pairs of the current word and the next words. We will create a mini batch from equally spaced pairs of words. This procedure corresponds to `(*)` in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        self.iteration = 0\n",
    "\n",
    "    def get_words(self):\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def __next__(self):\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Get current words that will be fed to RNN\n",
    "        cur_words = self.get_words()\n",
    "        self.iteration += 1\n",
    "        # Get next words that will be the target values.\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Penn Tree Bank long word sequence dataset\n",
    "# train/val/test is just an array of integers\n",
    "train, val, test = D.get_ptb_words()\n",
    "n_vocab = max(train) + 1\n",
    "\n",
    "# Get iterators of datasets\n",
    "batchsize = 20\n",
    "train_iter = ParallelSequentialIterator(train, batchsize)\n",
    "val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "test_iter = ParallelSequentialIterator(test, 1, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of a recurrent net for language modeling\n",
    "class RNNForLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNForLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        for param in self.params():\n",
    "            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h2 = self.l2(F.dropout(h1, train=self.train))\n",
    "        y = self.l3(F.dropout(h2, train=self.train))\n",
    "        return y\n",
    "    \n",
    "# Prepare an RNNLM model\n",
    "rnn = RNNForLM(n_vocab, 650)\n",
    "model = L.Classifier(rnn)\n",
    "model.compute_accuracy = False  # we only want the perplexity\n",
    "\n",
    "gpu = 1\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    model.to_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = O.SGD(lr=1.0)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and 6. Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most typical way of training a RNN is to unfold the RNN to regard it as a simple feed forward neural network (i.e. a computational graph without cycles) and do back propagation as usual. This procedure is known as **Back Propagation Through Time** (BPTT in short). But when the input sequence is long, BPTT is impossible because the whole data cannot fit into memory. In that case, we truncate the graph into short time ranges so that errors does not propagate too long in back propagation. This hurestic is known as **truncated Back Propagation Through Time** (truncated BPTT). \n",
    "\n",
    "To realize truncated BPTT in Chainer, we make a customized ``Updater``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        for i in range(self.bprop_len):\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # self.converter concatenates the word IDs to matrices and send them to the device\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "epoch = 20\n",
    "bproplen = 35\n",
    "updater = BPTTUpdater(train_iter, optimizer, bproplen, gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append an extension for evaluation with validation dataset.\n",
    "eval_model = model.copy()\n",
    "eval_rnn = eval_model.predictor\n",
    "eval_rnn.train = False\n",
    "\n",
    "trainer.extend(E.Evaluator(\n",
    "    val_iter, eval_model, device=gpu,\n",
    "    eval_hook=lambda _: eval_rnn.reset_state()))  # Reset the RNN state at the beginning of each evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append an extension for logging\n",
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])\n",
    "\n",
    "interval = 200\n",
    "trainer.extend(E.LogReport(postprocess=compute_perplexity,\n",
    "                           trigger=(interval, 'iteration')))\n",
    "\n",
    "trainer.extend(E.PrintReport(\n",
    "        ['epoch', 'iteration', 'perplexity', 'val_perplexity']\n",
    "), trigger=(interval, 'iteration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append an extension for saving training snapshots\n",
    "trainer.extend(E.snapshot())\n",
    "trainer.extend(E.snapshot_object(model, 'model_iter_{.updater.iteration}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       iteration   perplexity  val_perplexity\n",
      "\u001b[J0           200         1275.53                     \n",
      "\u001b[J0           400         569.226                     \n",
      "\u001b[J0           600         348.365                     \n",
      "\u001b[J0           800         287.389                     \n",
      "\u001b[J0           1000        273.528                     \n",
      "\u001b[J0           1200        236.117                     \n",
      "\u001b[J1           1400        223.513     203.803         \n",
      "\u001b[J1           1600        207.171                     \n",
      "\u001b[J1           1800        201.297                     \n",
      "\u001b[J1           2000        195.856                     \n",
      "\u001b[J1           2200        169.499                     \n",
      "\u001b[J1           2400        165.077                     \n",
      "\u001b[J1           2600        154.577                     \n",
      "\u001b[J2           2800        152.006     151.676         \n",
      "\u001b[J2           3000        158.183                     \n",
      "\u001b[J2           3200        148.043                     \n",
      "\u001b[J2           3400        145.232                     \n",
      "\u001b[J2           3600        138.266                     \n",
      "\u001b[J2           3800        133.03                      \n",
      "\u001b[J3           4000        137.28      128.511         \n",
      "\u001b[J3           4200        128.426                     \n",
      "\u001b[J3           4400        127.414                     \n",
      "\u001b[J3           4600        128.38                      \n",
      "\u001b[J3           4800        114.173                     \n",
      "\u001b[J3           5000        118.745                     \n",
      "\u001b[J3           5200        114.053                     \n",
      "\u001b[J4           5400        119.369     116.795         \n",
      "\u001b[J4           5600        122.034                     \n",
      "\u001b[J4           5800        108.277                     \n",
      "\u001b[J4           6000        108.252                     \n",
      "\u001b[J4           6200        102.692                     \n",
      "\u001b[J4           6400        103.934                     \n",
      "\u001b[J4           6600        100.494                     \n",
      "\u001b[J5           6800        108.82      108.077         \n",
      "\u001b[J5           7000        111.913                     \n",
      "\u001b[J5           7200        100.521                     \n",
      "\u001b[J5           7400        102.826                     \n",
      "\u001b[J5           7600        100.694                     \n",
      "\u001b[J5           7800        89.7741                     \n",
      "\u001b[J6           8000        95.8793     102.764         \n",
      "\u001b[J6           8200        92.196                      \n",
      "\u001b[J6           8400        97.774                      \n",
      "\u001b[J6           8600        105.919                     \n",
      "\u001b[J6           8800        92.107                      \n",
      "\u001b[J6           9000        99.0932                     \n",
      "\u001b[J6           9200        74.1572                     \n",
      "\u001b[J7           9400        87.5862     99.1323         \n",
      "\u001b[J7           9600        89.6371                     \n",
      "\u001b[J7           9800        90.5349                     \n",
      "\u001b[J7           10000       97.2333                     \n",
      "\u001b[J7           10200       83.8474                     \n",
      "\u001b[J7           10400       85.0352                     \n",
      "\u001b[J7           10600       85.0726                     \n",
      "\u001b[J8           10800       85.4034     96.1183         \n",
      "\u001b[J8           11000       82.6689                     \n",
      "\u001b[J8           11200       79.268                      \n",
      "\u001b[J8           11400       85.7117                     \n",
      "\u001b[J8           11600       83.7904                     \n",
      "\u001b[J8           11800       79.1044                     \n",
      "\u001b[J9           12000       86.4666     94.3489         \n",
      "\u001b[J9           12200       80.5511                     \n",
      "\u001b[J9           12400       77.43                       \n",
      "\u001b[J9           12600       80.671                      \n",
      "\u001b[J9           12800       73.6851                     \n",
      "\u001b[J9           13000       82.2141                     \n",
      "\u001b[J9           13200       76.1761                     \n",
      "\u001b[J10          13400       74.4194     92.8767         \n",
      "\u001b[J10          13600       74.3483                     \n",
      "\u001b[J10          13800       71.0058                     \n",
      "\u001b[J10          14000       76.8669                     \n",
      "\u001b[J10          14200       72.4569                     \n",
      "\u001b[J10          14400       70.3291                     \n",
      "\u001b[J10          14600       70.8652                     \n",
      "\u001b[J11          14800       73.5107     91.5574         \n",
      "\u001b[J11          15000       71.4234                     \n",
      "\u001b[J11          15200       73.3345                     \n",
      "\u001b[J11          15400       69.9214                     \n",
      "\u001b[J11          15600       73.3035                     \n",
      "\u001b[J11          15800       67.3216                     \n",
      "\u001b[J12          16000       69.5453     90.3294         \n",
      "\u001b[J12          16200       74.4994                     \n",
      "\u001b[J12          16400       76.0641                     \n",
      "\u001b[J12          16600       71.3187                     \n",
      "\u001b[J12          16800       64.9221                     \n",
      "\u001b[J12          17000       73.1468                     \n",
      "\u001b[J12          17200       64.8904                     \n",
      "\u001b[J13          17400       69.5748     89.9929         \n",
      "\u001b[J13          17600       72.6436                     \n",
      "\u001b[J13          17800       66.9043                     \n",
      "\u001b[J13          18000       68.1555                     \n",
      "\u001b[J13          18200       68.2714                     \n",
      "\u001b[J13          18400       66.377                      \n",
      "\u001b[J14          18600       70.788      89.3273         \n",
      "\u001b[J14          18800       68.7204                     \n",
      "\u001b[J14          19000       64.2783                     \n",
      "\u001b[J14          19200       62.0523                     \n",
      "\u001b[J14          19400       61.3952                     \n",
      "\u001b[J14          19600       71.3738                     \n",
      "\u001b[J14          19800       60.2558                     \n",
      "\u001b[J15          20000       65.2449     88.7517         \n",
      "\u001b[J15          20200       69.2869                     \n",
      "\u001b[J15          20400       61.3076                     \n",
      "\u001b[J15          20600       63.3673                     \n",
      "\u001b[J15          20800       59.5117                     \n",
      "\u001b[J15          21000       68.1604                     \n",
      "\u001b[J15          21200       57.9225                     \n",
      "\u001b[J16          21400       60.454      88.7397         \n",
      "\u001b[J16          21600       61.0273                     \n",
      "\u001b[J16          21800       63.1136                     \n",
      "\u001b[J16          22000       62.8917                     \n",
      "\u001b[J16          22200       64.1447                     \n",
      "\u001b[J16          22400       57.7086                     \n",
      "\u001b[J17          22600       55.7899     88.7746         \n",
      "\u001b[J17          22800       59.98                       \n",
      "\u001b[J17          23000       57.0623                     \n",
      "\u001b[J17          23200       63.0145                     \n",
      "\u001b[J17          23400       61.0263                     \n",
      "\u001b[J17          23600       67.1836                     \n",
      "\u001b[J17          23800       54.0169                     \n",
      "\u001b[J18          24000       61.8009     88.2561         \n",
      "\u001b[J18          24200       62.7301                     \n",
      "\u001b[J18          24400       59.5162                     \n",
      "\u001b[J18          24600       59.1596                     \n",
      "\u001b[J18          24800       61.5432                     \n",
      "\u001b[J18          25000       62.7427                     \n",
      "\u001b[J18          25200       57.4878                     \n",
      "\u001b[J19          25400       61.1725     88.2227         \n",
      "\u001b[J19          25600       63.1666                     \n",
      "\u001b[J19          25800       59.8086                     \n",
      "\u001b[J19          26000       55.4322                     \n",
      "\u001b[J19          26200       63.1896                     \n",
      "\u001b[J19          26400       58.8463                     \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Evaluation with validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 85.4273086004\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the final model with test dataset.\n",
    "eval_rnn.reset_state()\n",
    "evaluator = E.Evaluator(test_iter, eval_model, device=gpu)\n",
    "result = evaluator()\n",
    "print('test perplexity:', np.exp(float(result['main/loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
