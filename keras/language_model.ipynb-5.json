{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN example with Keras\n",
    "\n",
    "In this example, we build the language model made of stacked LSTMs with Penn Treebank dataset. This example is based on the official example in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes\n",
    "\n",
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theano\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K._BACKEND)\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5110)\n",
      "/home/delta/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# set gpu mode\n",
    "gpu = 0\n",
    "if gpu >= 0:\n",
    "    import theano.sandbox.cuda\n",
    "    theano.sandbox.cuda.use(\"gpu{}\".format(gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {}  # word to index map\n",
    "\n",
    "def load_data(filename):\n",
    "    global vocab\n",
    "    words = open(filename).read().replace('\\n', '<eos>').strip().split()\n",
    "    dataset = np.ndarray((len(words),), dtype=np.int32)\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "        dataset[i] = vocab[word]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "host = 'https://raw.githubusercontent.com'\n",
    "train_path = get_file('ptb.train.txt', '%s/tomsercu/lstm/master/data/ptb.train.txt' % host)\n",
    "train_data = load_data(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create training dataset from the raw dataset, which is a sequence of intergers. We sweep over the sequence and create a pair of input vector and target one by one. The length of the input vector equals to the BPTT length. The target of the input vector is its next word.\n",
    "\n",
    "<figure>\n",
    "<img src=\"../image/keras_rnn_minibatch.png\" alt=\"How to create training dataset\" style=\"width: 600px;\"/>\n",
    "<center>\n",
    "<figcaption>Fig. How to create a training dataset.</figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)\n",
    "maxlen = 40\n",
    "step = 10\n",
    "def vectorization(data):\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(data) - maxlen, step):\n",
    "        sentences.append(data[i: i + maxlen])\n",
    "        next_chars.append(data[i + maxlen])\n",
    "    X = np.zeros((len(sentences), maxlen, n_vocab), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char] = 1\n",
    "        y[i, next_chars[i]] = 1\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = vectorization(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare model and 4. Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Language model made of 2-layered stacked LSTMs.\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(vocab))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model and setup optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke the training with ``fit`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92955/92955 [==============================] - 505s - loss: 6.7020   \n",
      "Epoch 2/20\n",
      "92955/92955 [==============================] - 441s - loss: 6.4815   \n",
      "Epoch 3/20\n",
      "92955/92955 [==============================] - 437s - loss: 6.3300   \n",
      "Epoch 4/20\n",
      "92955/92955 [==============================] - 441s - loss: 6.1953   \n",
      "Epoch 5/20\n",
      "92955/92955 [==============================] - 441s - loss: 6.0821   \n",
      "Epoch 6/20\n",
      "92955/92955 [==============================] - 441s - loss: 5.9747   \n",
      "Epoch 7/20\n",
      "92955/92955 [==============================] - 441s - loss: 5.8654   \n",
      "Epoch 8/20\n",
      "92955/92955 [==============================] - 443s - loss: 5.7470   \n",
      "Epoch 9/20\n",
      "92955/92955 [==============================] - 441s - loss: 5.6295   \n",
      "Epoch 10/20\n",
      "92955/92955 [==============================] - 444s - loss: 5.5223   \n",
      "Epoch 11/20\n",
      "92955/92955 [==============================] - 440s - loss: 5.4100   \n",
      "Epoch 12/20\n",
      "92955/92955 [==============================] - 442s - loss: 5.3102   \n",
      "Epoch 13/20\n",
      "92955/92955 [==============================] - 442s - loss: 5.2182   \n",
      "Epoch 14/20\n",
      "92955/92955 [==============================] - 446s - loss: 5.1145   \n",
      "Epoch 15/20\n",
      "92955/92955 [==============================] - 447s - loss: 5.0049   \n",
      "Epoch 16/20\n",
      "92955/92955 [==============================] - 447s - loss: 4.9204   \n",
      "Epoch 17/20\n",
      "92955/92955 [==============================] - 451s - loss: 4.8081   \n",
      "Epoch 18/20\n",
      "92955/92955 [==============================] - 457s - loss: 4.7028   \n",
      "Epoch 19/20\n",
      "92955/92955 [==============================] - 456s - loss: 4.6326   \n",
      "Epoch 20/20\n",
      "92955/92955 [==============================] - 455s - loss: 4.5592   \n",
      "iter 0 train perplexity: 814.0428043021722\n",
      "iter 1 train perplexity: 652.9623733104008\n",
      "iter 2 train perplexity: 561.1781821869483\n",
      "iter 3 train perplexity: 490.4542914770054\n",
      "iter 4 train perplexity: 437.95272428308533\n",
      "iter 5 train perplexity: 393.3576466789062\n",
      "iter 6 train perplexity: 352.6067245767047\n",
      "iter 7 train perplexity: 313.2531883798757\n",
      "iter 8 train perplexity: 278.51471419265715\n",
      "iter 9 train perplexity: 250.21753953368673\n",
      "iter 10 train perplexity: 223.62396238462586\n",
      "iter 11 train perplexity: 202.3893494171646\n",
      "iter 12 train perplexity: 184.5988535163333\n",
      "iter 13 train perplexity: 166.42344457137844\n",
      "iter 14 train perplexity: 149.1398814389558\n",
      "iter 15 train perplexity: 137.05149982446648\n",
      "iter 16 train perplexity: 122.49352379969619\n",
      "iter 17 train perplexity: 110.25241208563146\n",
      "iter 18 train perplexity: 102.77863160919098\n",
      "iter 19 train perplexity: 95.50575311601354\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=128, nb_epoch=20)\n",
    "\n",
    "train_log_perp = np.array(history.history['loss'])\n",
    "train_perp = np.exp(train_log_perp)\n",
    "\n",
    "for i, t in enumerate(train_perp):\n",
    "    print('iter {} train perplexity: {}'.format(i, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the architecture and weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('language_model.json', 'w') as o:\n",
    "    o.write(model.to_json())\n",
    "model.save_weights('language_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
